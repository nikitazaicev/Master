\chapter{Conclusion}

The model trained on graphs collected from multiple datasets showed some level of “understanding” of the task at hand and in special cases managed to beat the greedy algorithm. The best results achieved by the approach presented in this work showed that \gls{gnn}s are capable of beating the greedy algorithm, but on average the performance was neither good nor consistent enough. The model on average achieved 90\% of what the greedy algorithm did, except for the rare cases where the greedy algorithm was further away from the optimal result. One of the reasons why our \gls{gnn} could not beat the greedy algorithm was due to the performance of the greedy algorithm being in most cases more than 90\% of the optimal result.

Applying models to the graphs intentionally made to challenge the greedy algorithm resulted in better performance, where the models outperform the greedy algorithm in some cases, yet fail at other subsets of graphs. Varying performance on different sets of graphs might be due to the lack of variety within the datasets used for training and the insufficient complexity of the model. Since the input graph can be virtually of any shape and form, it might be hard to cover all the possible cases. The use of our \gls{gnn} model might be better suited for the families of graphs that on average give worse results for greedy algorithm. However, such graphs were hard to find.

The models trained to specifically handle MNIST dataset graphs managed to outperform the greedy algorithm by 3\% with the line graph implementation and 1\% with the edge prediction, while the optimal result was on average only 6\% better than greedy. However, the graphs were so small that the Blossom algorithm was in fact faster, although the \gls{gnn}'s running time confidently outperforms Blossom algorithm as the graphs' sizes grow. The experiments on MNIST graphs strengthen the theory that a better use for such a model could be in cases where all the graphs belong to some subclass of graphs which narrows down the variety. 

\section{Future work}

The fact that our \gls{gnn} model developed in this work underperformed does not necessarily mean that the \gls{gnn}s are in general unfit for \gls{mwm} problem. There are several potential improvements that can be made.Various architectures and methods can be tested alongside larger datasets. Increasing the number of layers and neurons can enhance a model's ability to recognize complex patterns, albeit at the expense of longer training and prediction times. However, for this specific architecture, adding more layers had little to no impact. 

Another problem might be rooted in the supervised approach. Training a model based on the optimal solution has its pitfalls. The optimal solution can be completely different if even one edge is classified incorrectly. A semi-supervised or an unsupervised approaches are good potential candidates where precomputing the optimal solution would not be needed.